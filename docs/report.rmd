---
title: "Breaking free from the GIL"
subtitle: "Code: [`github.com/sueszli/nogil`](https://github.com/sueszli/nogil) \\hfill Yahya Jabary, Prof.\ Dr.\ Anton Ertl, TU Wien"
output: pdf_document
fontsize: 9pt
documentclass: article
papersize: a4
pagestyle: empty
geometry:
    - top=15mm
    - bottom=15mm
    - left=15mm
    - right=15mm
header-includes:
    - \pagenumbering{gobble}
    # title
    - \usepackage{titling}
    - \setlength{\droptitle}{-15pt}
    - \pretitle{\vspace{-30pt}\begin{center}\LARGE}
    - \posttitle{\end{center}\vspace{-20pt}}    
    # code
    - \usepackage{fancyvrb}
    - \fvset{fontsize=\fontsize{8pt}{8pt}\selectfont}
    - \usepackage{listings}
    - \lstset{basicstyle=\fontsize{8pt}{8pt}\selectfont\ttfamily}
    # code output
    - \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\fontsize{8pt}{8pt}}
    # tables
    - \usepackage{etoolbox}
    - \AtBeginEnvironment{longtable}{\footnotesize}
    - \AtBeginEnvironment{tabular}{\footnotesize}
    - \setlength{\LTleft}{0pt}
    - \setlength{\LTright}{0pt}
---

In October 2024, Python surpassed JavaScript for the first time as the most popular language on GitHub's Octoverse[^octo]. This high-level, interpreted, garbage-collected and dynamically-typed language emphasizes code readability and simplicity both in its implementation and syntax. This emphasis on simplicity also reflects itself in the language's community, often referred to as the "Pythonic" way of doing things.

However, the language's simplicity comes at a cost: While memory and network-bound tasks can be efficiently managed through colored functions[^color] and `asyncio`, Python is notoriously slow for compute-bound tasks due to the Global Interpreter Lock (GIL)[^gil].
The GIL is a mutex in Python's most popular implementation, CPython, that protects access to Python objects, preventing multiple threads from executing Python bytecodes simultaneously and therefore limits the language's performance on multi-core systems.

Or as Rob Pike put it in 2012[^go]:

> *"The computing landscape today is almost unrelated to the environment in which the languages being used, mostly C++, Java, and Python, had been created. The problems introduced by multicore processors, networked systems, massive computation clusters, and the web programming model were being worked around rather than addressed head-on. Moreover, the scale has changed: today's server programs comprise tens of millions of lines of code, are worked on by hundreds or even thousands of programmers, and are updated literally every day. To make matters worse, build times, even on large compilation clusters, have stretched to many minutes, even hours."*

These limitations have led to the development of various workarounds, such as the development of competing superset languages such as the `taichi`[^taichi] and `mojo`[^mojo], optimizing Python interpreters like `PyPy`[^pypy] and `Numba`[^numba], proposing to introduce multiple lightweight sub-interpreters[^subint1] [^subint2], or even making the GIL entirely optional[^nogil1] [^nogil2] [^nogil3], as proposed in PEP 703[^pep703].

The latter approach, making the GIL optional, was recently accepted in Python 3.13 and is currently in the experimental stage. There is no guarantee of this feature being included in the final release, but it is a step in the right direction.

In the past, parallelizing Python code was primarily achieved through the `multiprocessing` module, which, while effective, comes with significant drawbacks: it lacks shared memory support and consumes more resources compared to threading. 

The introduction of a GIL-free Python has raised hopes for more efficient parallelization. However, it's critical to recognize that Pythonâ€™s inherently dynamic nature and the overhead of its interpreter limit its performance far beyond the GIL. Even with these improvements, Python remains significantly slower - often by a factor of 1000 or more - compared to statically-typed systems languages[^bench].

This begs the question: why has Python become so dominant in scientific computing? The answer lies in its role as a high-level orchestration tool. In this domain, computational bottlenecks are offloaded to libraries written in optimized languages like C or Fortran, while Python acts as a glue language. This design is so entrenched that systems engineers frequently use macros to embed C code directly into Python[^nogildocs].

Recent developments in Python's concurrency capabilities, such as the removal of the GIL, are therefore not a revolution in raw performance but an enhancement in convenience. They make it easier for developers to write efficient code, especially for tasks that can benefit from parallel execution, without fundamentally altering Python's comparative speed limitations.

In this report, we will explore the optimization of a compute-bound task in Python across varying levels of abstraction. We will analyze the trade-offs between performance and usability, shedding light on how Python's evolving capabilities can be leveraged effectively.

# Algorithm: Naive Brute-Force Collision Attack

The embarrassingly parallel algorithm we have selected to demonstrate our optimizations, particularly the GIL-free multithreaded implementation, to put in contrast with the alternatives is a brute-force password cracker, most popular from the open-source library `hashcat`. The algorithm is simple: given a target hash, from a given character set and a maximum password length, the cracker generates all possible passwords and hashes them using the same algorithm. If the generated hash matches the target hash, the password is found and the program terminates.

To formalize, we define: character set $C$, maximum password length $n$, target hash value $h_{target}$, and hash function $H(x)$. 

The search space $S$ can be expressed as $S = \bigcup_{i=1}^n C^i$ where $C^i$ represents all possible strings of length $i$ from character set $C$. The problem can then be formulated as finding $p \in S$ such that: $H(p) = h_{target}$ and the total search space size is $|S| = \sum_{i=1}^n |C|^i$.

For parallel processing, we can partition $S$ into $k$ subsets: $S = \bigcup_{j=1}^k S_j$ where each $S_j$ can be processed independently by different threads.

To simplify the problem, we have chosen a small character set $C = \{a, \ldots z\}$ and a small maximum password length $n = 8$. The target hash value is set to `aaa` for simplicity. More sophisticated attacks would also consider the possibility of salted hashes, which we have omitted for brevity as well as clever algorithmic optimizations like bloom filters, rainbow tables, etc. which are beyond the scope of this report.

In initial experiments, we self-implemented 3 distinct hash functions $H(x)$ in Python, namely md5, sha1, and sha256, in plain Python to compare both the simplicity and security of the hash functions against our "collision attack". It's worth noting that none of these hash functions are considered secure for password hashing[^hash].

- `md5`: ~100 LoC, max 9847.03 hashes per second
- `sha1`: ~40 LoC, max 18578.26 hashes per second (approx. half as fast as the hashlib implementation)
- `sha256`: ~50 LoC, max 7870.21 hashes per second

To validate the correctness of our implementations, we have cross-validated the results with the `hashlib` library in Python. Due to the slow performance of any implementation other than our thoroughly optimized `sha1` implementation, we have decided to use it for the remainder of our experiments - unless otherwise noted.

# Methodology: Different Levels of Abstraction

To benchmark our experiments with the experimental GIL-free CPython version 13t (threaded), we implemented a Docker container that builds Python from source and sets the correct compile flags to disable the GIL. The container additionally serves to increase isolation and the reproducibility of our experiments.

#### 1. Plain Python

The most straightforward implementation of the brute-force attack is a simple loop that iterates over all possible passwords and hashes them. We have implemented this in three different ways: (1) completely free of any libraries, (2) using the `itertools` library to improve succinctness and (3) using the `hashlib` library for hashing (in just 4 LoC).

#### 2. Multiprocessing

The `multiprocessing` module in Python allows for parallel processing by creating separate processes for each task. We have implemented the brute-force attack using the `map`, `imap`, `map_async`, and `imap_unordered` functions to compare the performance of different parallelization strategies. We omitted the `starmap` function as it is just syntactic sugar for `map` and the Executor API as the overhead of creating a new process for each task is too high.

When using the `multiprocessing` library in Python, we can call multiple system processes that each come with their own separate Python interpreter, GIL, and memory space. This is very simple, straightforward, and the intended way to write parallel code in the latest Python version. But it comes with all the pros and cons of using processes for parallel programming:

- Simple, has higher isolation, security and robustness.
- Context switching: actually doesn't matter, since the threading library threads are kernel-level as well.
- Resource overhead: memory allocation, creation, and management are slower for processes. Additionally, having a unique copy of the interpreter for each process is really wasteful.
- Serialization overhead: there is no shared memory, so data has to be serialized and deserialized for inter-process communication. Also, some objects are unserializable: the pickle module is used to serialize objects. But some objects are not pickleable (i.e. lambdas, file handles, etc.).

#### 3. Multithreading

The `threading` module in Python allows for parallel processing by creating separate threads for each task. With the GIL-free Python, this level of abstraction is expected to experience the most significant performance improvements. We have implemented the brute-force attack using the `ThreadPoolExecutor` and `Thread` classes to compare the performance of different parallelization strategies. While the `ThreadPoolExecutor` is more convenient, the `Thread` class allows for more fine-grained control over the threads and resembles the join API of C more closely.

When using the `threading` library in Python with the GIL released using the `GIL=0` flag, we achieve the lowest overhead and the highest performance of any pure Python implementation for parallel processing.

#### 4. CTypes

CTypes is a foreign function interface (FFI) for Python that allows calling functions from shared libraries.

We initially implemented our own optimized Sha1 implementation in C, but noticed that it performs almost identically to the `openssl/sha.h` implementation. We have therefore decided to use the more robust implementation instead to reduce complexity in our experiments. We conducted two experiments: one in plain C and one using OpenMP to parallelize the hashing function.

This method is the most straightforward way to call C code from Python and requires close to no knowledge of the Python C API[^cpython]. The GIL is released automatically on each foreign function call. However, it comes with massive serialization overhead as automatic type conversions done by the FFI-library are very expensive. This can be partially circumvented by passing pointers or using CFFI but it will still be significantly slower than extending CPython. Ideally one should share as little data, pass as little data and call the foreign function as little as possible.

We did so by only passing the target hash $h_{target}$ to the C function and returning the password if it matches.

Ctypes aren't meant to be used for high performance libraries that you use frequently but codebase-glue. you can still use them for that purpose and gain a significant amount of performance, but you have to move as much of the computation as possible into the C implementation.

#### 5. CPython

Finally, we have implemented the brute-force attack by extending the CPython interpreter directly. Given our lessons from previous experiments, we have decided to use the `openssl/sha.h` implementation for hashing and leave out the OpenMP parallelization.

Extending CPython has neglible to no overhead and allows to share large chunks of memory by calling `mmap()` directly. However, it comes with a very complex API and requires you to manually manage the GIL with `Py_BEGIN_ALLOW_THREADS` and `Py_END_ALLOW_THREADS` macros and marshal all data passed between the C and Python code. It also isn't portable and requires a lot of boilerplate code to build and distribute.

# Results

We beat `hashlib` by 13.525ns (2.5x) or 101,703,681 instructions (3.3x). This was achieved through the `ctypes` library, CPython-C-API and various C libraries.

|gil   |type            |command                                | instructions (med)| task_clock (med) | user_time (med)| sys_time (med) |
|:-----|:---------------|:--------------------------------------|-------------------:|-----------------:|----------------:|---------------:|
|true  |**cpython**         |**invoke_hashcat.py (openmp)** |            44442376|             8.760|        0.0090265|       0.0000000|
|true  |ctypes          |invoke_hashcat.py (openmp) |            80107280|            39.820|        0.0160960|       0.0000000|
|true  |ctypes          |invoke_hashcat.py        |           118592997|            16.110|        0.0162195|       0.0000000|
|true  |**plain**           |**lib.py (hashlib libary)**                                |           146146058|            22.285|        0.0222055|       0.0000000|
|false |multithreading  |workers.py                             |           198008716|            39.985|        0.0258195|       0.0113530|
|true  |multithreading  |workers.py                             |          1030919157|           106.765|        0.1006565|       0.0111120|
|true  |plain           |itertools.py                           |          3945750392|           325.575|        0.3242800|       0.0000000|
|true  |plain           |improved.py                            |          3959326962|           322.015|        0.3206755|       0.0000000|
|true  |plain           |plain.py                               |          4752510454|           400.205|        0.3996415|       0.0000000|
|true  |multiprocessing |imap.py                                |          6620723294|          1743.685|        1.2987810|       0.4785180|
|true  |multiprocessing |imap_unordered.py                      |          6692752894|          1787.350|        1.3024570|       0.5340625|
|true  |multithreading  |executor.py                            |        241741072306|         20136.600|       19.8526395|       0.6276710|
|false |multithreading  |executor.py                            |        241749347062|         63354.890|       63.2586825|       0.0327220|
|true  |multiprocessing |map_async.py                           |        244913585430|         61218.555|       61.0370955|       0.2066270|
|true  |multiprocessing |map.py                                 |        245013383854|         61259.295|       61.0844710|       0.2048880|

More importantly, disabling the GIL while individually managing threads with the `multithreading` API reduced time spent on syscalls by 42x. This shows that the GIL is a significant bottleneck for parallel computing in Python. Nonetheless, the single threaded implementation invoking the `hashlib` library, our `ctypes` implementation or the CPython extension are most likely the best choice for most performance critical applications.

All experiments were conducted on an 11th Gen Intel Core i7-1165G7 processor running at 2.80 GHz with 16 GB of RAM.

These insights provide a valuable foundation for future work in optimizing Python performance and offer practical guidance for developers seeking to enhance the efficiency of their applications.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
deps <- c("ISLR", "Metrics", "ggplot2", "tidyr", "dplyr", "patchwork", "gridExtra", "knitr")
for (p in deps) {
    if (!requireNamespace(p, quietly = TRUE)) {
        install.packages(p, repos = "https://cran.rstudio.com")
    }
    library(p, character.only = TRUE)
}

set.seed(42)
options(scipen=999)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=15, fig.height=10}
df <- read.csv("./data_v2_edited.csv")

tmp_df <- df[, !(names(df) %in% c("time_elapsed", "cycles"))]
tmp_df <- tmp_df %>%
    group_by(gil, type, command) %>%
    summarise_all(list(median = median)) %>%
    arrange(instructions_median)

tmp_df <- tmp_df[, !(names(tmp_df) %in% c("task_clock_median", "user_time_median", "sys_time_median"))]
tmp_df <- tmp_df[order(-tmp_df$instructions_median),]

ggplot(tmp_df, aes(x = reorder(ifelse(gil == "false", paste("(GIL disabled)", type, "â€“", command), paste(type, "â€“", command)), -instructions_median), y = instructions_median, group = 1)) +
    geom_point(size = 4) +
    geom_line(size = 1) +
    geom_hline(data = tmp_df %>% filter(type == "plain" & command == "lib.py"), aes(yintercept = instructions_median), color = "red", linetype = "dashed", size = 1.5) +
    annotate("text", x = 1, y = tmp_df %>% filter(type == "plain" & command == "lib.py") %>% pull(instructions_median), label = "standard library", color = "red", vjust = -1, hjust = -1, size = 6) +
    scale_y_log10(labels = scales::comma) +
    labs(
        x = "GIL, Optimization Type, Command",
        y = "Instructions Median",
        title = "Median Instructions per Command"
    ) +
    theme_minimal() +
    theme(
        axis.text.x = element_text(angle = 90, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12),
        axis.title = element_text(size = 14),
        plot.title = element_text(size = 16)
    )
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=15, fig.height=10}
df <- read.csv("./data_v2_edited.csv")

tmp_df <- df[, !(names(df) %in% c("time_elapsed", "cycles"))]
tmp_df <- tmp_df %>%
    group_by(gil, type, command) %>%
    summarise_all(list(median = median)) %>%
    arrange(instructions_median)

tmp_df <- tmp_df[, !(names(tmp_df) %in% c("instructions_median", "user_time_median", "sys_time_median"))]
tmp_df <- tmp_df[order(-tmp_df$task_clock_median),]

ggplot(tmp_df, aes(x = reorder(ifelse(gil == "false", paste("(GIL disabled)", type, "â€“", command), paste(type, "â€“", command)), -task_clock_median), y = task_clock_median, group = 1)) +
    geom_point(size = 4) +
    geom_line(size = 1) +
    geom_hline(data = tmp_df %>% filter(type == "plain" & command == "lib.py"), aes(yintercept = task_clock_median), color = "red", linetype = "dashed", size = 1.5) +
    annotate("text", x = 1, y = tmp_df %>% filter(type == "plain" & command == "lib.py") %>% pull(task_clock_median), label = "standard library", color = "red", vjust = -1, hjust = -1, size = 6) +
    scale_y_log10(labels = scales::comma) +
    labs(
        x = "GIL, Optimization Type, Command",
        y = "Task Clock Median",
        title = "Median task clock per command"
    ) +
    theme_minimal() +
    theme(
        axis.text.x = element_text(angle = 90, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12),
        axis.title = element_text(size = 14),
        plot.title = element_text(size = 16)
    )
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5}
df <- read.csv("./data_v1.csv")

ggplot(df, aes(y = reorder(command, median), x = median)) +
    geom_segment(aes(x = min, xend = max, yend = command)) +
    geom_point(aes(x = min), shape = "|", size = 4) +
    geom_point(aes(x = max), shape = "|", size = 4) +
    geom_point(aes(x = median), shape = 18, size = 3, color = "red") +
    geom_text(aes(x = max + 0.05, label = sprintf("User: %.3f\nSystem: %.3f", user, system)), hjust = +0.5, size = 2) +
    labs(title = "Median Execution time per Command", x = "Median Execution Time (s)", y = "Command") +
    theme(axis.text.y = element_text(size = 8), plot.margin = margin(r = 100, unit = "pt")) +
    scale_x_continuous(expand = expansion(mult = c(0.05, 0.3))) +
    theme_light()
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=20, fig.height=10}
df <- read.csv("./data_v2_edited.csv")

# time_elapsed = also system load, I/O waiting time, context switching, interference by other processes, ... -> only useful for user experience
df <- df[, !(names(df) %in% c("time_elapsed"))]

# task_clock = user_time + system_time
# - user_time = code efficiency -> user space, code efficiency
# - system_time = kernel operation overhead -> kernel space, Process management, Inter-process communication, Context switching, System calls
df <- df[, !(names(df) %in% c("user_time", "sys_time"))]

# cycles also measure NOPs or waiting cores, so it can be misleading
df <- df[, !(names(df) %in% c("cycles"))]

# group by id and get median
df <- df %>%
    group_by(gil, type, command) %>%
    summarise_all(list(median = median)) 

# ggplot(df, aes(x = task_clock_median, y = instructions_median, color = type)) +
#     geom_point(size = 5) +
#     geom_text(aes(label = command), hjust = -0.1, size = 5) +
#     geom_point(data = df %>% filter(gil == "false"), aes(x = task_clock_median, y = instructions_median), shape = 1, size = 7, color = "black") +
#     scale_x_log10(labels = scales::comma) +
#     scale_y_log10(labels = scales::comma) +
#     scale_color_brewer(palette = "Set2") +
#     labs(
#         x = "Task Clock (ms)",
#         y = "Instructions",
#         color = "Type",
#         caption = "A ring means GIL = false"
#     ) +
#     theme_light() +
#     theme(
#         legend.position = "bottom",
#         plot.margin = margin(1, 3, 1, 1, "cm"),
#         axis.text = element_text(size = 14),
#         axis.title = element_text(size = 16),
#         legend.text = element_text(size = 14),
#         legend.title = element_text(size = 16),
#         plot.title = element_text(size = 18),
#         plot.caption = element_text(size = 14, hjust = 0.5)
#     )
```

[^octo]: https://github.blog/news-insights/octoverse/octoverse-2024/#the-most-popular-programming-languages
[^gil]: Wang, Z., Bu, D., Sun, A., Gou, S., Wang, Y., & Chen, L. (2022). An empirical study on bugs in python interpreters. IEEE Transactions on Reliability, 71(2), 716-734.
[^color]: https://langdev.stackexchange.com/questions/3430/colored-vs-uncolored-functions
[^go]: https://go.dev/talks/2012/splash.article
[^bench]: https://benchmarksgame-team.pages.debian.net/benchmarksgame/index.html
[^taichi]: https://www.taichi-lang.org/
[^mojo]: https://docs.modular.com/mojo/stdlib/python/python.html
[^pypy]: https://www.pypy.org/
[^numba]: https://numba.pydata.org/
[^pep703]: https://peps.python.org/pep-0703/
[^subint1]: https://peps.python.org/pep-0554/
[^subint2]: https://peps.python.org/pep-0683/
[^nogil1]: https://peps.python.org/pep-0703/
[^nogil2]: https://discuss.python.org/t/a-steering-council-notice-about-pep-703-making-the-global-interpreter-lock-optional-in-cpython/30474
[^nogil3]: https://engineering.fb.com/2023/10/05/developer-tools/python-312-meta-new-features/
[^nogildocs]: https://docs.python.org/3/c-api/init.html#releasing-the-gil-from-extension-code
[^hash]: https://cheatsheetseries.owasp.org/cheatsheets/Password_Storage_Cheat_Sheet.html
[^cpython]: https://github.com/python/cpython/blob/main/Include/Python.h
